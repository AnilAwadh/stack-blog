---
layout: post
title: The Perfect Web Spider Storm
date: 2009-06-15 23:46:33.000000000 -04:00
categories:
- background
- server
tags: []
status: publish
type: post
published: true
meta:
  _edit_last: '1'
  _podPressPostSpecific: a:6:{s:15:"itunes:subtitle";s:15:"##PostExcerpt##";s:14:"itunes:summary";s:15:"##PostExcerpt##";s:15:"itunes:keywords";s:17:"##WordPressCats##";s:13:"itunes:author";s:10:"##Global##";s:15:"itunes:explicit";s:2:"No";s:12:"itunes:block";s:2:"No";}
author:
  login: admin
  email: jatwood@codinghorror.com
  display_name: Jeff Atwood
  first_name: Jeff
  last_name: Atwood
excerpt: !ruby/object:Hpricot::Doc
  options: {}
---
<p>
We noticed something unusual on our <a href="http://www.cacti.net/">Cacti</a> graphs today. Can you spot it?</p>
<p>
<img src="assets/stackoverflow-cacti-graph-june-15-2009.png" alt="stackoverflow-cacti-graph-june-15-2009" title="stackoverflow-cacti-graph-june-15-2009" /></p>
<p>
Yes! The light gray of the graph background <i>does</i> seem a few shades lighter than normal! I see it too!</p>
<p>
No, no, of course I'm talking about that massive traffic spike from 06:00 to 15:00 PST (server time). In <a href="http://www.youtube.com/watch?v=3nyCIqJP_4s">the words of The Office's David Brent</a>:</p>
<blockquote><p>
I think there's been a rape up there!
</p></blockquote>
<p>
Bandwidth isn't usually a problem for us, as we are heavily text-oriented and go to great lengths to <a href="http://blog.stackoverflow.com/2009/02/happy-100000th-question/">make sure all our text content is served up compressed</a>. This is almost 3x our normal <i>peak</i> traffic level. And for what?</p>
<p>
Geoff ran a few queries in <a href="http://www.microsoft.com/downloads/details.aspx?FamilyID=890cd06b-abf8-4c25-91b2-f8d975cf8c07&displaylang=en">Log Parser</a> and found that this is yet another instance of <b>a perfect web spider storm</b>. Here are the top 3 bandwidth consumers in the logs for that day:</p>
<table width="600">
<tr>
<td><b>IP</b></td>
<td><b>User-Agent</b></td>
<td align="right"><b>Requests</b></td>
<td align="right"><b>Bytes Served</b></td>
</tr>
<tr>
<td>72.30.78.240</td>
<td><a href="http://help.yahoo.com/help/us/ysearch/slurp">Yahoo! Slurp/3.0</a></td>
<td align="right">56,331</td>
<td align="right">1,124,861,780</td>
</tr>
<tr>
<td>66.249.68.109</td>
<td><a href="http://www.google.com/bot.html">Googlebot/2.1</a></td>
<td align="right">56,579</td>
<td align="right">773,418,834</td>
</tr>
<tr>
<td>66.249.68.109</td>
<td>Mediapartners-Google</td>
<td align="right">30,519</td>
<td align="right">671,904,609</td>
</tr>
</table>
<p>
As I mentioned, this has happened to us before -- and we've considered <a href="http://serverfault.com/questions/20383/dynamically-blocking-excessive-http-bandwith-use">dynamically blocking excessive HTTP bandwidth use</a>. But first we politely asked the Yahoo and Google web spider bots to play a bit nicer:</p>
<ol>
<li>We updated our <a href="http://stackoverflow.com/robots.txt">robots.txt</a> to include the <code>Crawl-delay</code> directive, like so:</p>
<pre>
User-agent: Slurp
Crawl-delay: 1

User-agent: msnbot
Crawl-delay: 1
</pre>
</li>
<li>We went to <a href="https://www.google.com/webmasters/tools/">Google Webmaster Tools</a> and told Google to send no more than 2 Googlebot search engine spider requests per second.
</li>
</ol>
<p>
That was a week ago. Obviously, it isn't working.</p>
<p>
Now we'll have to do this the hard way. Fortunately, Geoff (aka <a href="http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/">Valued Stack Overflow Associate #00003</a>) has a "spare" <a href="http://www.cisco.com/en/US/products/hw/vpndevc/ps2030/ps4094/">Cisco PIX 515E</a> laying around that we plan to put in front of the web servers, so we can dynamically throttle the offenders. But we can't do that for a week or so. </p>
<p>
In the meantime, since Yahoo (via Slurp!) is about 0.3% of our traffic, but insists on rudely consuming a huge chunk of our prime-time bandwidth, <b>they're getting IP banned and blocked</b>. I'm a bit more sympathetic to Google, since <a href="http://www.codinghorror.com/blog/archives/001224.html">they deliver almost 90% of our traffic</a>, but it sure would be nice if they'd allow me to at least schedule the massive web spider storms for off-peak hours...</p>
<p /></p></p></p></p></p></p>
